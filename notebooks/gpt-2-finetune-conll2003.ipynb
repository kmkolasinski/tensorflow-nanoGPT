{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(dataset[\"train\"])\n",
    "NER_TAGS = {\n",
    "    \"O\": 0,\n",
    "    \"B-PER\": 1,\n",
    "    \"I-PER\": 2,\n",
    "    \"B-ORG\": 3,\n",
    "    \"I-ORG\": 4,\n",
    "    \"B-LOC\": 5,\n",
    "    \"I-LOC\": 6,\n",
    "    \"B-MISC\": 7,\n",
    "    \"I-MISC\": 8,\n",
    "}\n",
    "NER_TAGS_INV = {v: k for k, v in NER_TAGS.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "train_data = []\n",
    "for data in tqdm(dataset[\"train\"]):\n",
    "    tags = [NER_TAGS_INV[tag] for tag in data[\"ner_tags\"]]\n",
    "    train_data.append((data[\"tokens\"], tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = []\n",
    "for data in tqdm(dataset[\"validation\"]):\n",
    "    tags = [NER_TAGS_INV[tag] for tag in data[\"ner_tags\"]]\n",
    "    validation_data.append((data[\"tokens\"], tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.quantile([len(data[0]) for data in train_data], 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(item):\n",
    "    targets_data = []\n",
    "    current_ner = []\n",
    "\n",
    "    for token, tag_name in zip(*item):\n",
    "\n",
    "        if tag_name.startswith(\"B-\"):\n",
    "            if len(current_ner) == 0:\n",
    "                current_ner.append((token, tag_name[2:]))\n",
    "                continue\n",
    "            else:\n",
    "                targets_data.append(current_ner)\n",
    "                current_ner = []\n",
    "                current_ner.append((token, tag_name[2:]))\n",
    "                continue\n",
    "\n",
    "        if tag_name.startswith(\"I-\") and len(current_ner) > 0:\n",
    "            current_ner.append((token, tag_name[2:]))\n",
    "            continue\n",
    "\n",
    "        if len(current_ner) > 0:\n",
    "            targets_data.append(current_ner)\n",
    "            current_ner = []\n",
    "\n",
    "    if len(current_ner) > 0:\n",
    "        targets_data.append(current_ner)\n",
    "\n",
    "\n",
    "    context = \" \".join(item[0])\n",
    "    \n",
    "    target_text = \"\"\n",
    "    for tokens in targets_data:\n",
    "        words = \" \".join([word for word, tag in tokens])\n",
    "        target_text += f\"{words}//{tokens[0][1]}\\n\"\n",
    "\n",
    "    return context, target_text\n",
    "\n",
    "\n",
    "item = train_data[5]\n",
    "for token, tag_name in zip(*item):\n",
    "    print(f\"{token:15}{tag_name}\")\n",
    "\n",
    "context_text, target_text = prepare_data(item)\n",
    "\n",
    "print(f\"prompt:{context_text}\")\n",
    "print(f\"target:{target_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_train_data = [prepare_data(item) for item in tqdm(train_data)]\n",
    "prepared_validation_data = [prepare_data(item) for item in tqdm(validation_data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build tf.data.Dataset iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_nano_gpt.model import GPT2Tokenizer\n",
    "\n",
    "sequence_length = 256\n",
    "context_length =  sequence_length // 2\n",
    "tokenizer = GPT2Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_lm_targets(context, target):\n",
    "    context_tokens, target_tokens = tokenizer.tokenize_sample(context, target)\n",
    "    context_tokens = tokenizer.pad_or_slice(context_tokens, context_length)\n",
    "    target_tokens = tokenizer.pad_or_slice(target_tokens, context_length + 1)\n",
    "\n",
    "    x = tf.concat([context_tokens, target_tokens[:-1]], 0)\n",
    "    y = tf.concat([context_tokens, target_tokens[1:]], 0)\n",
    "\n",
    "    # simple mask to remove context from the loss computation as well as pad tokens\n",
    "    mask = tf.cast(tf.abs(x - y) > 0, tf.int32)\n",
    "    targets_ids = tf.stack([y, mask], 1)\n",
    "\n",
    "    return {\"inputs_ids\": x, \"targets_ids\": targets_ids}, targets_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset_iterator(prepared_data, is_training: bool, batch_size: int) -> tf.data.Dataset:\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(prepared_data)\n",
    "    dataset = dataset.map(\n",
    "        lambda x: prepare_lm_targets(x[0], x[1]), num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "\n",
    "    if is_training:\n",
    "        dataset = dataset.repeat(-1)\n",
    "        dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_dataset = build_dataset_iterator(prepared_train_data, is_training=True, batch_size=batch_size)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = build_dataset_iterator(prepared_validation_data, is_training=False, batch_size=batch_size)\n",
    "validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_dataset:\n",
    "    break\n",
    "\n",
    "x = x[\"inputs_ids\"]\n",
    "pad_str = tokenizer.pad_token_str\n",
    "print(tokenizer.detokenize(x[0, :context_length]).numpy().decode(\"utf-8\").replace(pad_str, \"\"))\n",
    "print(tokenizer.detokenize(y[0, context_length:, 0]).numpy().decode(\"utf-8\").replace(pad_str, \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFGPT2Model\n",
    "from tf_nano_gpt.model import freeze_embeddings, freeze_layers\n",
    "from tf_nano_gpt.metrics import masked_lm_loss, masked_accuracy\n",
    "\n",
    "base_model = TFGPT2Model.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_embeddings(base_model)\n",
    "freeze_layers(base_model, num_blocks_to_freeze=8, use_lora=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict_fn(\n",
    "    inputs_ids: tf.Tensor,\n",
    "    past_key_values: tf.Tensor = None,\n",
    "    position_ids: tf.Tensor = None,\n",
    "):\n",
    "    encoded_input = {\n",
    "        \"input_ids\": inputs_ids,\n",
    "        \"attention_mask\": tf.ones_like(inputs_ids),\n",
    "        \"past_key_values\": past_key_values,\n",
    "        \"position_ids\": position_ids,\n",
    "    }\n",
    "\n",
    "    output = base_model(encoded_input)\n",
    "    last_hidden_state = output.last_hidden_state\n",
    "    past_key_values = output.past_key_values\n",
    "\n",
    "    last_hidden_state = tf.keras.layers.Dropout(0.2)(last_hidden_state)\n",
    "\n",
    "    logits = base_model.transformer.wte(last_hidden_state, mode=\"linear\")\n",
    "    return logits, past_key_values\n",
    "\n",
    "\n",
    "inputs_ids = tf.keras.layers.Input(\n",
    "    shape=(sequence_length,), dtype=tf.int32, name=\"inputs_ids\"\n",
    ")\n",
    "targets_ids = tf.keras.layers.Input(\n",
    "    shape=(sequence_length, 2), dtype=tf.int32, name=\"targets_ids\"\n",
    ")\n",
    "\n",
    "logits, _ = model_predict_fn(inputs_ids)\n",
    "\n",
    "loss_value = masked_lm_loss(targets_ids, logits)\n",
    "accuracy_value = masked_accuracy(targets_ids, tf.argmax(logits, -1))\n",
    "\n",
    "train_model = tf.keras.Model(inputs=[inputs_ids, targets_ids], outputs=logits)\n",
    "\n",
    "train_model.add_loss(loss_value)\n",
    "train_model.add_metric(accuracy_value, name=\"accuracy\")\n",
    "\n",
    "train_model.summary(180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model = tf.keras.Model(inputs=inputs_ids, outputs=logits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "steps_per_epoch = 1000\n",
    "save_dir = \"models/test-gpt-2-model-v1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "decay_steps = steps_per_epoch * epochs\n",
    "validation_steps = validation_dataset.cardinality().numpy() // batch_size\n",
    "validation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n",
    "train_model.compile(optimizer, jit_compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "lr_scheduler = tf.keras.optimizers.schedules.CosineDecay(0.0001, epochs, alpha=0.01)\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "  learning_rate = lr_scheduler(epoch)\n",
    "  tf.summary.scalar('learning rate', data=learning_rate, step=epoch)\n",
    "  return learning_rate\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=Path(save_dir) / \"logs\",\n",
    "        histogram_freq=0,\n",
    "        embeddings_freq=0,\n",
    "        update_freq=\"epoch\",\n",
    "        write_steps_per_second=True,\n",
    "        profile_batch=(200, 250),\n",
    "        write_graph=False,\n",
    "    ),\n",
    "    tf.keras.callbacks.LearningRateScheduler(lr_schedule),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        Path(save_dir) / \"model\",\n",
    "        monitor = \"val_accuracy\",\n",
    "        verbose = 1,\n",
    "        save_best_only = True,\n",
    "        save_weights_only = True,\n",
    "        mode = \"auto\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model.load_weights(Path(save_dir) / \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_dataset:\n",
    "    break\n",
    "\n",
    "y_pred = train_model(x)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=validation_dataset,\n",
    "    validation_steps=validation_steps,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks,\n",
    "    epochs=epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset_iter = iter(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_predict_next_token(inputs_ids: tf.Tensor) -> tf.Tensor:\n",
    "    \n",
    "    current_index = tf.reduce_sum(tf.cast(inputs_ids > 0, tf.int32), -1) - 1\n",
    "    num_sentences, maxlen = tf.shape(inputs_ids)[0], tf.shape(inputs_ids)[1]\n",
    "\n",
    "    y = inference_model(inputs_ids)\n",
    "    logits = tf.gather(y, current_index, batch_dims=1)\n",
    "    sampled_indices = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "\n",
    "    current_index = tf.minimum(current_index + 1, maxlen - 1)\n",
    "    scatter_indices = tf.stack([tf.range(num_sentences), current_index], axis=1)\n",
    "\n",
    "    inputs_ids = (\n",
    "        tf.scatter_nd(scatter_indices, sampled_indices, shape=(num_sentences, maxlen))\n",
    "        + inputs_ids\n",
    "    )\n",
    "    return inputs_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(validation_dataset_iter)\n",
    "x = x['inputs_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "x_test = tf.concat([x[:, :context_length + 1], tf.zeros_like(x)[:, context_length + 1:]], -1)\n",
    "context, target = tokenizer.detokenize(x)[idx].numpy().decode().split(tokenizer.start_token_str)\n",
    "target_text = tokenizer.detokenize(y[:, context_length:, 0])[idx].numpy().decode()\n",
    "\n",
    "print(context.replace(tokenizer.pad_token_str, \"\"))\n",
    "print(target_text.replace(tokenizer.pad_token_str, \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_token = tokenizer.stop_token_id\n",
    "for i in range(context_length):\n",
    "    x_test = greedy_predict_next_token(x_test)\n",
    "    all_complete = tf.shape(tf.unique(tf.where(x_test[:, context_length:] == stop_token)[:, 0]).y) == batch_size\n",
    "    if bool(all_complete[0].numpy()):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tokenizer.detokenize(x_test[:, context_length + 1:])[idx].numpy().decode().replace(\"!\", \"\")\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling using keras_nlp functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_nlp\n",
    "\n",
    "\n",
    "@tf.function(input_signature=[tf.TensorSpec(shape=(None, None), dtype=tf.int32)])\n",
    "def token_probability_fn(inputs):\n",
    "\n",
    "    input_len = tf.shape(inputs)[1]\n",
    "\n",
    "    inputs = tf.map_fn(\n",
    "        lambda _: tokenizer.pad_or_slice(_, sequence_length), inputs\n",
    "    )\n",
    "\n",
    "    y = inference_model(inputs)\n",
    "    return y[:, input_len - 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = x[idx, : context_length + 1][None, :]\n",
    "\n",
    "predicted_tokens = keras_nlp.utils.top_k_search(\n",
    "    token_probability_fn,\n",
    "    prompt,\n",
    "    max_length=sequence_length,\n",
    "    end_token_id=tokenizer.stop_token_id,\n",
    "    from_logits=True,\n",
    "    k=5,\n",
    ")\n",
    "\n",
    "if len(predicted_tokens.shape) == 1:\n",
    "    predicted_tokens = predicted_tokens[None, :]\n",
    "predicted_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tokenizer.detokenize(predicted_tokens[:, context_length + 1:])[0].numpy().decode().replace(\"!\", \"\")\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export model - base method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Exporter(tf.Module):\n",
    "    def __init__(\n",
    "        self, model: tf.keras.Model, tokenizer: GPT2Tokenizer, jit_compile: bool = None\n",
    "    ):\n",
    "        super(GPT2Exporter, self).__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.jit_compile = jit_compile\n",
    "        self.predict_next_token_fn = greedy_predict_next_token\n",
    "\n",
    "        if jit_compile:\n",
    "            self.predict_next_token_fn = tf.function(\n",
    "                greedy_predict_next_token, jit_compile=True\n",
    "            )\n",
    "\n",
    "    def prepare_inputs(self, text: str):\n",
    "        input_ids = self.tokenizer.tokenize(text)\n",
    "        input_ids = self.tokenizer.pad_or_slice(input_ids, context_length)\n",
    "        input_ids = self.tokenizer.pad_or_slice(\n",
    "            tf.concat([input_ids, [self.tokenizer.start_token_id]], 0),\n",
    "            sequence_length,\n",
    "            pad_value=0,\n",
    "        )\n",
    "\n",
    "        input_ids = tf.reshape(input_ids, [1, -1])\n",
    "\n",
    "        return input_ids\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec([], tf.string)])\n",
    "    def __call__(self, text):\n",
    "        input_ids = self.prepare_inputs(text)\n",
    "\n",
    "        i = tf.constant(0)\n",
    "        while i < context_length:\n",
    "            i += 1\n",
    "            input_ids = self.predict_next_token_fn(input_ids)\n",
    "\n",
    "            completed = tf.reduce_any(input_ids == self.tokenizer.stop_token_id)\n",
    "\n",
    "            if completed:\n",
    "                break\n",
    "\n",
    "        prediction = self.tokenizer.detokenize(input_ids[:, : context_length + i - 1])\n",
    "        prediction = tf.strings.split(prediction, self.tokenizer.start_token_str)[0, 1]\n",
    "\n",
    "        return {\"outputs\": prediction}\n",
    "\n",
    "\n",
    "gpt2_predictor = GPT2Exporter(inference_model, tokenizer)\n",
    "gpt2_predictor_jit = GPT2Exporter(inference_model, tokenizer, jit_compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Headingley is a suburb of Leeds, West Yorkshire, England, approximately two miles out of the city centre, to the north west along the A660 road. Headingley is the location of the Beckett Park campus of Leeds Beckett University and Headingley Stadium.\"\n",
    "\n",
    "prediction = gpt2_predictor(prompt)\n",
    "prediction['outputs'].numpy().decode().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = gpt2_predictor_jit(prompt)\n",
    "prediction['outputs'].numpy().decode().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit gpt2_predictor(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit gpt2_predictor_jit(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(gpt2_predictor, Path(save_dir) / \"exported-models/gpt2-ner/1/\")\n",
    "tf.saved_model.save(gpt2_predictor_jit, Path(save_dir) / \"exported-models/gpt2-ner-jit/1/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export model with cached keys and queries\n",
    "This is applicable only for GPT2 small which has 12 blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_argmax(logits):\n",
    "    return tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "\n",
    "\n",
    "class CachedGPT2Exporter(GPT2Exporter):\n",
    "    def __init__(\n",
    "        self, model: tf.keras.Model, tokenizer: GPT2Tokenizer, jit_compile: bool = False\n",
    "    ):\n",
    "        super(CachedGPT2Exporter, self).__init__(model, tokenizer, jit_compile=False)\n",
    "        self.predict_next_token_fn = model_predict_fn\n",
    "        if jit_compile:\n",
    "            self.predict_next_token_fn = tf.function(model_predict_fn, jit_compile=True)\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec([], tf.string)])\n",
    "    def __call__(self, text):\n",
    "        input_ids = self.prepare_inputs(text)\n",
    "        input_ids = input_ids[:, : context_length + 1]\n",
    "\n",
    "        logits, past_key_values = self.predict_next_token_fn(input_ids)\n",
    "        new_input_ids = tf.expand_dims(sample_argmax(logits[:, -1, :]), axis=-1)\n",
    "\n",
    "        states = tf.TensorArray(tf.int32, size=context_length)\n",
    "        states = states.write(0, new_input_ids)\n",
    "\n",
    "        # for autograph to compile this function we need to specify each variable explicitly\n",
    "        # since set_loop_options does not woth with python lists, that's why this function\n",
    "        # will work only with GPT2 small model\n",
    "        kv0 = past_key_values[0]\n",
    "        kv1 = past_key_values[1]\n",
    "        kv2 = past_key_values[2]\n",
    "        kv3 = past_key_values[3]\n",
    "        kv4 = past_key_values[4]\n",
    "        kv5 = past_key_values[5]\n",
    "        kv6 = past_key_values[6]\n",
    "        kv7 = past_key_values[7]\n",
    "        kv8 = past_key_values[8]\n",
    "        kv9 = past_key_values[9]\n",
    "        kv10 = past_key_values[10]\n",
    "        kv11 = past_key_values[11]\n",
    "\n",
    "        for i in tf.range(context_length - 1):\n",
    "            tf.autograph.experimental.set_loop_options(\n",
    "                shape_invariants=[\n",
    "                    (kv0, tf.TensorShape([2, 1, 12, None, 64])),\n",
    "                    (kv1, tf.TensorShape([2, 1, 12, None, 64])),\n",
    "                    (kv2, tf.TensorShape([2, 1, 12, None, 64])),\n",
    "                    (kv3, tf.TensorShape([2, 1, 12, None, 64])),\n",
    "                    (kv4, tf.TensorShape([2, 1, 12, None, 64])),\n",
    "                    (kv5, tf.TensorShape([2, 1, 12, None, 64])),\n",
    "                    (kv6, tf.TensorShape([2, 1, 12, None, 64])),\n",
    "                    (kv7, tf.TensorShape([2, 1, 12, None, 64])),\n",
    "                    (kv8, tf.TensorShape([2, 1, 12, None, 64])),\n",
    "                    (kv9, tf.TensorShape([2, 1, 12, None, 64])),\n",
    "                    (kv10, tf.TensorShape([2, 1, 12, None, 64])),\n",
    "                    (kv11, tf.TensorShape([2, 1, 12, None, 64])),\n",
    "                ]\n",
    "            )\n",
    "            past_key_values = [\n",
    "                kv0,\n",
    "                kv1,\n",
    "                kv2,\n",
    "                kv3,\n",
    "                kv4,\n",
    "                kv5,\n",
    "                kv6,\n",
    "                kv7,\n",
    "                kv8,\n",
    "                kv9,\n",
    "                kv10,\n",
    "                kv11,\n",
    "            ]\n",
    "\n",
    "            past_length = i + context_length - 1\n",
    "            position_ids = tf.expand_dims(\n",
    "                tf.range(past_length, past_length + 1), axis=0\n",
    "            )\n",
    "\n",
    "            logits, new_past_key_values = self.predict_next_token_fn(\n",
    "                inputs_ids=new_input_ids,\n",
    "                past_key_values=past_key_values,\n",
    "                position_ids=position_ids,\n",
    "            )\n",
    "            new_input_ids = tf.expand_dims(sample_argmax(logits[:, -1, :]), axis=-1)\n",
    "            states = states.write(i + 1, new_input_ids)\n",
    "\n",
    "            kv0 = new_past_key_values[0]\n",
    "            kv1 = new_past_key_values[1]\n",
    "            kv2 = new_past_key_values[2]\n",
    "            kv3 = new_past_key_values[3]\n",
    "            kv4 = new_past_key_values[4]\n",
    "            kv5 = new_past_key_values[5]\n",
    "            kv6 = new_past_key_values[6]\n",
    "            kv7 = new_past_key_values[7]\n",
    "            kv8 = new_past_key_values[8]\n",
    "            kv9 = new_past_key_values[9]\n",
    "            kv10 = new_past_key_values[10]\n",
    "            kv11 = new_past_key_values[11]\n",
    "\n",
    "            completed = tf.reduce_any(new_input_ids == self.tokenizer.stop_token_id)\n",
    "\n",
    "            if completed:\n",
    "                break\n",
    "\n",
    "        input_ids = tf.reshape(states.stack(), [1, -1])[:, :i]\n",
    "\n",
    "        prediction = self.tokenizer.detokenize(input_ids)\n",
    "\n",
    "        return {\"outputs\": prediction[0]}\n",
    "\n",
    "\n",
    "gpt2_cached_predictor = CachedGPT2Exporter(\n",
    "    inference_model, tokenizer, jit_compile=False\n",
    ")\n",
    "gpt2_cached_predictor_jit = CachedGPT2Exporter(\n",
    "    inference_model, tokenizer, jit_compile=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = gpt2_cached_predictor(prompt)\n",
    "prediction['outputs'].numpy().decode().replace(\"!\", \"\").split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = gpt2_cached_predictor_jit(tf.constant(prompt))\n",
    "prediction['outputs'].numpy().decode().replace(\"!\", \"\").split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit gpt2_cached_predictor(tf.constant(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit gpt2_cached_predictor_jit(tf.constant(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(gpt2_cached_predictor, Path(save_dir) / \"exported-models/gpt2-ner-cached/1/\")\n",
    "tf.saved_model.save(gpt2_cached_predictor_jit, Path(save_dir) / \"exported-models/gpt2-ner-cached-jit/1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.saved_model.load(\"models/test-gpt-2-model/exported-models/gpt2-ner-cached-jit/1\")\n",
    "loaded_model.signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test tensorflow Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "cwd = Path.cwd()\n",
    "save_dir = \"models/test-gpt-2-model\"\n",
    "\n",
    "run_serving_cmd = f\"docker run -p 8501:8501 --rm --gpus all --name tfserving_models --mount type=bind,source={cwd}/{save_dir}/exported-models/gpt2-ner,target=/models/model -e MODEL_NAME=model -t tensorflow/serving:2.11.1-gpu\"\n",
    "print(run_serving_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "context = \"Headingley is a suburb of Leeds, West Yorkshire, England, approximately two miles out of the city centre, to the north west along the A660 road. Headingley is the location of the Beckett Park campus of Leeds Beckett University and Headingley Stadium.\"\n",
    "\n",
    "prediction_url = \"http://localhost:8501/v1/models/model:predict\"\n",
    "post_data = {\"inputs\": {\"text\": context}}\n",
    "response = requests.post(prediction_url, data=json.dumps(post_data))\n",
    "prediction = response.json()[\"outputs\"]\n",
    "prediction.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision2",
   "language": "python",
   "name": "vision2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
